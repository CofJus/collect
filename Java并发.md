## 1 Thread

### 1.1 Java线程状态转换

![](https://cdn.tobebetterjavaer.com/stutymore/thread-state-and-method-20230829143200.png)

### 1.2 阻塞、等待、超时等待的区别

|          | BLOCKED（阻塞）                     | WAITING（等待）           | TIMED_WAITING（超时等待）               |
| -------- | ----------------------------------- | ------------------------- | --------------------------------------- |
| 进入原因 | 竞争资源（**锁**）或I/O（文件读写） | 主动放弃CPU资源           | 主动放弃CPU资源 **并设置超时时间**      |
| 转变原因 | 被竞争资源的资源得到释放            | 需要其他线程唤醒          | 需要其他线程唤醒/**等待超时后自动唤醒** |
| 关键字   | synchronized/lock..                 | wait/notify,park/unpark.. | sleep,wait(long)/notify,parkNanos..     |

### 1.3 sleep和wait的区别

- 定义位置不同：sleep是Thread方法，wait是Object方法；

- 调用地方不同：sleep方法可以在任何地方使用；wait方法则只能和synchronized一起使用；

- 锁资源释放方式不同：sleep方法只让出了CPU，不释放锁； wait方法让出锁，只有调用了`notify`方法，之前调用wait的线程才会解除wait状态，可以去参与竞争同步资源锁；

- 异常捕获：sleep需要捕获或者抛出异常，而wait/notify/notifyAll则不需要；

### 1.4 Java线程和操作系统线程的关系

Java 线程状态是在操作系统线程状态的基础上进行的抽象和封装。Java 线程的运行最终还是要依赖操作系统的线程调度和执行机制。

Java线程模型简化了操作系统的线程模型。

Java中的线程和操作系统的线程并非一一对应，JVM可以将多个Java线程的任务分配给一个操作系统线程。

## 2 Java内存模型（JMM）

JMM（Java Memory Model）主要聚焦于多线程环境下的内存访问规则，是对 JVM 内存管理的一种抽象和规范补充。

它涵盖的主题包括变量的可见性、指令重排、原子操作等，旨在解决由于多线程并发编程带来的一些问题。

### 2.1 happens-before原则

面向开发者而非编译器，happens-before在多线程程序的时间轴上建立了一个先后顺序的约束。

指令重排必须遵守这些规则。

- 程序顺序规则：代码顺序执行，前面的代码 happens-before后面的代码

- 管程锁定规则：加锁 happens-before 解锁 （monitorenter happens-before monitorexit）
- volatile变量规则：写happens-before读
- 线程启动规则：start方法happens-before其他任何操作
- 传递性规则：顾名思义

### 2.2 工作内存

JMM则规定，在线程共享的主内存之外，每个线程持有一块私有的工作内存。

主内存负责存储对象实例、静态变量等数据，而线程对变量的操作（如读取、赋值等）都在自己的工作内存中进行。

JMM 中的主内存概念可以对应到 JVM 中的堆、方法区等共享内存区域，线程的工作内存则可以看作是线程栈以及部分缓存机制（如 CPU 缓存）的抽象结合。

Java 内存模型定义了包括read、write、lock、unlock、load、store等8种同步操作，来保证工作内存和主内存的正确交互。

### 2.3 JMM和JVM运行时内存区域

简而言之，一个抽象，一个具体。

JMM针对的是多线程环境下，如何在主内存与工作内存之间安全地执行操作。它是一种规范，一个模型。

JVM运行时内存区域则是具体详细的内存划分，比如堆、栈、方法区等等。

JMM 定义的规则需要通过 JVM 来具体实现。JVM 在实现过程中会利用硬件的内存架构和自身的内存管理机制来满足 JMM 的要求。

## 3 volatile

volatile是Java对内存屏障的实现，保证变量的可见性和禁止指令重排序

### 3.1 可见性

可见性是指当一个线程修改了共享变量的值后，其他线程能够立即察觉到这个修改并获取到最新的值。

**破坏可见性**

- 缓存：计算机多级缓存
- 指令重排：多线程环境下，重排序可能会导致一个线程看到的共享变量的状态不符合预期

### 3.2 重排序

**引申**：CPU缓存一致性协议MESI，通过广播失效来保证多CPU的缓存一致性。同时为了避免阻塞带来性能问题，将缓存失效异步处理。

- 编译器指令重排：编译器层面的优化，对没有依赖关系的代码进行重排
- CPU指令重排：CPU层面的优化，提高并行度
- 内存系统重排：写缓冲区和失效队列异步处理缓存失效，带来指令执行顺序和写入内存顺序不一致的问题

### 3.3 内存屏障

本质是CPU指令，作用有两个

- 顺序性：禁止屏障两侧，即屏障这条CPU指令两侧的指令重排
- 一致性：强制刷新内存和失效其他CPU的缓存

三个指令

- 读屏障（Load）：确保在该屏障之后的读操作能够看到屏障之前的所有写操作的结果。
- 写屏障（Store）：保证在该屏障之前的写操作先于屏障之后的写操作执行，并且将屏障之前的写操作的结果刷新到主内存中。
- 全屏障（Full）：同时保证读写操作的可见性和顺序性

组合出四种内存屏障：StoreStore,LoadLoad,LoadStore,StoreLoad

StoreLoad最通用，屏障左边的写操作会被立即刷入主存，屏障右边的读操作会从主存读最新的值。从而保证了可见性。

### 3.4 volatile实现

对于被volatile修饰的变量

写操作之前：加StoreStore屏障，保证当前写操作一定是在之前的写操作生效的基础上进行的，保证两次写操作的顺序性。

写操作之后：加StoreLoad屏障，保证当前写操作产生的结果对之后的读操作可见。

读操作之前：加LoadLoad屏障，保证在当前读操作之前的读操作不会被重排到后面。

读操作之后：加LoadStore屏障，保证在读操作之后的写操作不会被重排到读操作之前。

### 3.5 volatile和原子性

volatile只作用于变量的单个读/写操作，对于复合操作如自增自减无能为力。

## 4 synchronized

### 4.1 Monitor锁对象

**Synchronized可以把任何一个非null对象作为"锁"**

作用方式不同时，Monitor对象也不同

- 同步代码块：锁住给定的对象
- 同步方法：锁对象实例this
- 同步静态方法：锁住class对象

### 4.2 同步原理

synchronized的同步依赖monitorenter和monitorexit两条指令实现，前者加锁，后者退出锁。

**两个指令的执行是JVM通过调用操作系统的互斥原语mutex来实现，被阻塞的线程会被挂起、等待重新调度**，会导致用户态和内核态两个态之间来回切换，对性能有较大影响。

### 4.3 MESA管程模型

管程的主要目的是将共享资源的管理和对这些资源的操作封装在一起，使得对共享资源的访问能够以一种安全、有序的方式进行。在管程内部，有一套规则来确保在同一时刻只有一个线程能够访问共享资源。

Java的同步synchronized和AQS均基于MESA管程模型实现。

**共享变量封装：**在 MESA 管程模型中，共享变量被定义在管程内部，外部线程无法直接访问这些变量，只能通过管程提供的接口方法来操作。对于synchronized，共享变量就是Monitor锁对象，对外提供wait、notify等方法操作Monitor对象。

**入口队列：**当多个线程试图进入管程时，会在管程的入口处形成一个队列，称为入口队列。入口队列的存在确保了线程按照一定的顺序（通常是先来先服务，但也可以根据具体实现有所不同）进入管程。在Java中，同时只有一个对象能获取锁，被阻塞的线程类似于入口队列机制，在Monitor对象之外等待。

**条件变量和等待队列：**用于让线程在某个条件不满足时等待，直到其他线程使这个条件满足并唤醒等待的线程。每个条件变量都与一个等待队列相关联，线程可以在这个等待队列中等待特定条件的满足。用Java代入这个模型，wait释放当前锁进入等待状态，这个状态类似于进入等待队列。

### 4.4 锁优化

#### 4.4.1 自适应自旋锁

**线程的阻塞和唤醒需要CPU从用户态转为核心态**，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作，势必会给系统的并发性能带来很大的压力。同时我们发现在许多应用上面，**对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的**。所以引入自旋锁，当一个线程尝试获取某个锁时，**如果该锁已被其他线程占用，就一直循环检测锁是否被释放**，而不是进入线程挂起或睡眠状态。

自旋必须有一个限度，无限自旋是对CPU资源的浪费。

引入自适应自旋次数：自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多，反之减少。

#### 4.4.2 锁消除

有些情况下，JVM检测到不可能存在共享数据竞争（逃逸分析），这是JVM会对这些同步锁进行锁消除。

#### 4.4.3 锁粗化

如果对锁的使用过于精细，可能会导致性能下降。锁粗化就是将多个连续的、对同一个锁的操作合并为一个范围更大的锁操作，从而减少获取和释放锁的次数，以提高性能。

场景：循环加锁，JVM会把锁提到循环外。

#### 4.4.4 锁膨胀

无锁->偏向锁->轻量级锁->重量级锁

#### 4.4.5 偏向锁

**在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得。**引入偏向锁，降低线程加锁的代价。偏向锁非常适合在单线程环境下或者在多线程环境中，一个对象的同步块主要被一个线程频繁访问，而其他线程很少访问的情况。

**加锁：**检查Java对象头中Mark Word中的偏向锁标志位，若无偏向锁则写入当前的线程id从而获得锁；若有则检查当前的偏向锁线程id，相等则直接获取锁，不等则膨胀为轻量级锁。

**解锁：**同步代码块执行结束释放；当其他线程来竞争时，JVM在Safe Point暂停当前持有偏向锁的线程，然后根据这个线程同步代码执行状态决定是释放锁到无锁状态还是膨胀到轻量级锁；

#### 4.4.6 轻量级锁

**加锁：**膨胀为轻量级锁之后，JVM会先在当前线程的**栈桢**中创建**锁记录空间**，并将对象头中的Mark Word复制到锁记录中，然后线程尝试将对象头中的Mark Word的数据**CAS**为**指向锁记录的指针**。如果成功，当前线程获得锁，如果失败，则自旋获取锁，当自旋获取锁仍然失败时，表示存在两条或两条以上的线程竞争同一个锁，则轻量级锁会膨胀成重量级锁。

**解锁：**尝试把栈帧锁空间中存储的Mark Word通过CAS放回对象头，成功则回归无锁状态或者再次偏向；失败则代表锁已被其他线程持有，不做任何操作。

#### 4.4.7 重量级锁

依赖于操作系统Mutex Lock实现，涉及线程的阻塞和唤醒，以及操作系统的调度，开销最大。

避免了自旋对CPU资源的消耗，适用于多线程竞争非常激烈，并且线程持有锁的时间较长的场景。

## 5 CAS

CAS是一种乐观锁，将比较和赋值两个操作封装起来，在硬件层面保证原子性。

Java的CAS依赖unsafe包中提供的native方法实现。

### 5.1 CAS自旋

一种在多线程环境下使用 CAS（Compare - And - Swap）操作时的策略。当一个线程执行 CAS 操作尝试更新共享变量但失败时，它不会立即进入阻塞状态，而是会在一个循环中不断地再次尝试执行 CAS 操作，这个循环过程就被称为 CAS 自旋。

**优势**

- 快速响应共享变量的变化
- 快速响应共享变量的变化

**可能存在的问题**

- CPU资源消耗：自旋是CPU空转，次数过多是对CPU的浪费，需要一定的策略控制次数，类比synchronized悲观锁的自适应自旋。
- 无法保证公平性：可能会造成某些线程长时间自旋无法成功更新。

### 5.2 CAS在Java中应用

- 原子类（Atomic）的实现基础
- 无锁队列，例如Disruptor利用CAS写环形缓冲
- AQS中用CAS操作状态变量，以实现加锁和解锁，保证可重入

### 5.3 ABA问题

以一个链表为例，初始链表A->B->null，线程1期望将A和B交换，此时线程2先行一步移除元素B并在后面插入了C、D两个元素，链表变成A->C->D->null，B->null，然后线程1再进来CAS时发现链表头为A，CAS成功，于是链表变为B->null，这样C、D两个元素就消失了。

**解决办法：**CAS比较时同时比较版本号和时间戳

### 5.4 CAS和总线风暴

CAS 操作本质上是一种硬件级别的原子操作，在多处理器系统中，当多个处理器核心尝试对共享内存中的同一数据进行 CAS 操作时，需要通过系统总线来保证数据的一致性。CAS同时涉及总线读和写操作，高并发场景下，会影响总线的通信效率，导致系统响应变慢。

## 6 AQS

AQS 是一个用于构建锁和同步器的基础**框架**。它提供了一种实现阻塞式同步容器的通用机制，抽象出了同步状态的获取和释放操作，并且通过维护一个同步队列（FIFO 队列）来管理等待获取同步状态的线程。

### 6.1 AQS核心

- **同步状态state**：这是 AQS 的核心变量，通常是一个整型值，用于表示同步状态。例如，在`ReentrantLock`中，`state`可以表示锁是否被占用（0 表示未被占用，大于 0 表示被占用，且可以用于记录重入次数）；在`CountDownLatch`中，`state`表示还需要等待的事件数量。这个变量的具体含义和操作方式由 AQS 的子类根据不同的同步需求来定义。

- **同步队列（CLH 队列）**：AQS 内部维护的一个双向链表结构的队列，用于存放等待获取同步状态的线程。每个节点（Node）代表一个等待线程，节点中包含了线程引用、等待状态等信息。当线程尝试获取同步状态失败时，就会被封装成一个节点添加到同步队列的尾部，然后进入等待状态。这个队列的操作（如入队、出队、等待唤醒等）是 AQS 实现线程阻塞和唤醒机制的基础。

  **头节点一定代表当前持有锁的线程。**

- **线程等待状态waitStatus**：CLH队列中节点（等待的线程）的等待状态。可能的取值有五种。
  
  - **初始值（0）**：节点初始入队时的默认值，无意义
  - **已取消 CANCELLED（1）**：线程已超时/中断，即不再等待同步状态
  - **后继待唤醒 SIGNAL（-1）**：当前节点的线程退出同步状态时，唤醒下一个节点。新节点入队时，将前一个节点置为SIGNAL。
  - **条件等待 CONDITION（-2）**：结合Condition说明。
  - **传播唤醒 PROPAGATE（-3）**：结合Semaphore说明。


### 6.2 AQS模板方法

AQS提供了几个非抽象的模板方法，子类可以按需实现。

- **tryAcquire**：独占式获取同步状态。
- **tryRelease**：独占式释放同步状态。
- **tryAcquireShared**：共享方式尝试获取资源。
- **tryReleaseShared**：共享方式尝试释放资源。

### 6.3 AQS和管程

AQS和synchronized一样，都可以视作Java对管程模型的实现。

ReentrantLock和synchronized的诸多相似之处都可以归结到管程模型上。

AQS的很多概念可以在管程模型中找到对应

- state变量 - 管程互斥
- CLH队列 - 入口队列
- ConditionObject - 条件等待

## 7 ReentrantLock

### 7.1 可重入

ReentrantLock实现可重入：通过AQS的state记录重入次数，当前线程每次获取锁，通过CAS使state+1；释放锁时-1，直到state清零，锁才真正释放。

可重入的意义：避免同一个线程尝试多次获取同一个锁时发生死锁（如递归、嵌套）

### 7.2 公平锁

初始化时显式地指定公平锁。相较于非公平锁，线程在获取锁时会先检查，如果当前CLH队列中有线程在等待&&当前线程不是队首元素，则获取锁失败。简而言之是利用了队列FIFO的特性，实现了线程的公平竞争。

实现层面，ReentrantLock内部维护了一个继承AQS的同步器Sync，Sync有两个子类FairSync和NonFairSync，分别对应公平/非公平锁。

### 7.3 Condition

Condition是一个接口，用于在多线程环境下实现更精细的线程等待和唤醒机制。它是配合锁（如ReentrantLock）一起使用的，提供了类似于Object类中的wait、notify和notifyAll方法的功能，但比它们更加灵活和强大。

Condition内部维护了一个单向FIFO等待队列，AQS只有一个同步队列，而Condition把同步队列扩展至多个，从而实现更灵活的同步需求

- await：自动释放当前持有的锁，将自己封装成一个节点添加到当前Condition的等待队列中。

  线程会暂停执行，直到被其他线程调用signal唤醒。

- signal：唤醒在Condition队列中的一个线程。

## 8 LockSupport

[LockSupport详解](https://pdai.tech/md/java/thread/java-thread-x-lock-LockSupport.html)

LockSupport提供了基本的线程同步原语，底层是一系列native方法。

主要有park和unpark两个方法。

- park：阻塞当前线程，放弃CPU资源，线程进入waiting状态，不释放锁
- unpark：唤醒指定线程

### 8.1 LockSupport为什么高效

- 轻量：通过调用操作系统的底层原语来实现的，相比synchronized复杂的monitor机制更低级，更轻量。
- 灵活：可以在任意位置调用，不需要和synchronized搭配使用
- 精准：相比wait/signal，LockSupport可以精准唤醒任意线程

## 9 ThreadLocal

ThreadLocal本质是线程私有的一个Map。

主要用于在多线程环境下，避免多个线程共享变量时可能出现的数据竞争和并发安全问题。

### 9.1 内部实现

- **内部数据结构**：ThreadLocal 内部维护了一个以 ThreadLocal 对象为键，以变量副本为值的哈希表。每个线程对象（Thread）内部也有一个类似的哈希表（ThreadLocalMap），用于存储该线程对应的 ThreadLocal 变量副本。
- **变量的存储与访问**：当一个线程首次访问一个 ThreadLocal 变量时，会在自己的 ThreadLocalMap 中创建一个键值对，键是这个 ThreadLocal 对象，值是该变量的初始副本。后续该线程每次访问这个 ThreadLocal 变量时，就会从自己的 ThreadLocalMap 中通过这个 ThreadLocal 对象作为键来获取对应的变量副本。在这个示例中，`thread1`和`thread2`分别设置和获取`threadLocalVariable`的值，它们不会相互干扰，因为每个线程都有自己独立的变量副本。

### 9.2 可能的内存泄漏

- **内存泄漏产生的原因**：在 ThreadLocal 的实现中，每个 Thread 对象内部的 ThreadLocalMap 中的键值对是**弱引用（Weak Reference）**形式存储 ThreadLocal 对象。当一个 ThreadLocal 对象没有其他强引用时，它可能会被垃圾回收。但是，其对应的变量副本（值）在 ThreadLocalMap 中仍然可能存在，如果没有手动清除这些值，就可能会导致内存泄漏。
- **解决方法**：为了避免内存泄漏，通常需要在使用完 ThreadLocal 变量后，手动调用`remove`方法来清除对应的变量副本。

## 10 ConcurrentHashMap

线程安全的HashMap

### 10.1 HashMap线程不安全的点

HashMap用拉链法解决hash冲突，线程不安全主要来自对链表的修改。

- 插入（put）：并发插入链表可能会破坏链表结构。jdk1.7之前的头插法还有可能带来循环链表问题，从而导致数据丢失/死循环。
- 扩容（resize）：扩容过程中涉及到元素迁移，将旧桶中的键值对重新hash到新的桶，如果这个过程是并发的，可能会出现同一个桶重复映射或者没有被迁移的情况。
- 删除（remove）：并发删除链表可能导致链表断裂，数据丢失。

### 10.2 ConcurrentHashMap怎样解决线程安全问题

- volatile：ConcurrentHashMap桶里的元素Node都用volatile修饰，保证修改可见性。

- 插入（put）
  - 无hash冲突：CAS插入
  - 有hash冲突：synchronized锁桶。后续在链表/红黑树上的任何操作都因为已经锁桶而不用担心线程安全问题。
- 查找（get）：没有写操作，无锁并发
- 扩容（resize）
  - sizeCtl：>0时为当前Map的容量，<0时表示当前正处于扩容阶段，绝对值就是正在进行扩容的线程数
  - transferIndex
    - CAS操作transferIndex，用于给正在执行扩容的线程动态分配任务区间，如线程A负责区间[100,200]的桶的数据迁移。
    - 结合sizeCtl指示的线程数，线程越多，分配给单个线程的任务就越少，从而提高数据迁移的并发度
  - ForwardingNode：当一个桶正在进行迁移时，这个桶对应的节点会被设置为ForwardingNode。告诉其他线程这个桶正在迁移过程中。对于读取操作的线程，当遇到ForwardingNode时，就知道需要到新数组中去查找元素，因为旧桶中的元素正在被移动到新位置。
  - synchronized：迁移过程中，对正在迁移的桶加锁。
- 计数（size）
  - CounterCell数组：和每个桶一一对应，记录每个桶里的元素总数，CAS操作加减，将put/remove时发生的数量变化分散到每个桶。获取容量时需要累加整个数组。
  - 扩容期间：不一定准确，只提供弱一致性的结果

## 11 CopyOnWriteList

一个线程安全的List。

核心思想是在对集合进行修改操作（如添加、删除、修改元素）时，会复制一份原有的数组，在新的数组上进行修改操作，然后将原数组引用指向新数组，从而实现对集合的修改。而读取操作（如`get`）则可以在原数组上进行，不需要加锁，因为在修改操作时会生成新的数组副本，所以读取操作不会受到修改操作的影响，这就保证了读取操作的高效性和并发安全性。

简而言之，有写锁无读锁，适用于读多写少的场景。

存在的问题

- 数据一致性：可能读不到最新的数据
- 内存占用：因为要维护一个副本，相当于内存翻倍了

## 12 BlockingQueue

阻塞队列（Blocking Queue）在普通队列的基础上增加了阻塞特性。当队列已满或者为空时，执行插入（如`put`操作）或者获取（如`take`操作）操作的线程会被阻塞，直到队列状态允许操作继续进行。

应用场景：生产者消费者模型

### 12.1 ArrayBlockingQueue

- **数据结构**：基于数组实现的有界阻塞队列。它在创建时需要指定队列的容量，并且这个容量在队列的生命周期内是固定不变的。
- **同步机制**：内部通过重入锁（ReentrantLock）和两个条件对象（Condition）来实现线程之间的同步。一个条件对象用于控制队列满时生产者线程的等待，另一个条件对象用于控制队列空时消费者线程的等待。

### 12.2 LinkedBlockingQueue

- **数据结构**：基于链表实现的阻塞队列，可以是有界的也可以是无界的（默认是无界的，若指定容量则为有界）。因为基于链表，它在插入和删除元素时相对更加灵活，不需要像 ArrayBlockingQueue 那样预先分配固定大小的数组空间。
- **同步机制**：内部也使用了锁和条件对象来实现同步。不过，它通常有更高的并发性能，因为它在插入和获取元素时使用了两个不同的锁（putLock 和 takeLock），可以允许同时进行插入和获取操作（在一定条件下），提高了吞吐量。

## 13 ThreadPool

线程池



[☆啃碎并发（七）：深入分析Synchronized原理](https://www.jianshu.com/p/e62fa839aa41)

[ABA问题例子](https://blog.csdn.net/WSYW126/article/details/53979918)

[线程池](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)
